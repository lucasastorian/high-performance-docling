#!/usr/bin/env python3
"""
Test script to verify CUDA Graphs fixes for correctness and performance
"""

import os
import torch
import time
from pathlib import Path

# Simple test data generation
def create_test_data(B=2, C=256, H=28, W=28, S=784, D=512):
    """Create test data matching expected shapes"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # Encoder output: [B, C, H, W]
    enc_out_batch = torch.randn(B, C, H, W, device=device)
    
    # Memory encoding: [S, B, D] 
    mem_enc = torch.randn(S, B, D, device=device)
    
    return enc_out_batch, mem_enc

def test_correctness():
    """Test that graph and eager paths produce identical results"""
    print("=== Testing Correctness ===")
    
    # Set deterministic seed
    torch.manual_seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(42)
    
    # Import the decoder (assuming it's available)
    try:
        from fork.table.batched_graph_decoder import BatchedTableDecoderV2
        
        # Mock model for testing
        class MockModel:
            def __init__(self):
                self._max_pred_len = 32
                self._bbox = False  # Disable bbox for simpler testing
                
                # Mock tag transformer
                class MockTT:
                    def __init__(self):
                        self._decoder_dim = 512
                        self._embedding = torch.nn.Embedding(100, 512).cuda()
                        self._fc = torch.nn.Linear(512, 100).cuda()
                        
                        # Mock PE
                        class MockPE:
                            def __init__(self):
                                self.pe = torch.randn(1024, 512).cuda()
                        
                        class MockPositionalEncoding:
                            def __init__(self):
                                self.pe = torch.randn(1024, 512).cuda()
                        
                        self._positional_encoding = MockPositionalEncoding()
                        
                        # Mock decoder layers
                        self._decoder = type('MockDecoder', (), {'layers': []})()
                        
                    def step_fullprefix(self, t, tgt_emb_buf, memory=None, cache=None, 
                                       memory_kv=None, sa_kv_cache=None, max_pred_len=None):
                        B = tgt_emb_buf.size(1)
                        D = tgt_emb_buf.size(2)
                        # Return mock hidden state
                        last_H = torch.randn(B, D, device=tgt_emb_buf.device)
                        return last_H, None, sa_kv_cache
                    
                    def precompute_mem_kv(self, mem_enc):
                        return []
                
                self._tag_transformer = MockTT()
                
                # Mock init data
                word_map = {
                    "<start>": 0, "<end>": 1, "fcel": 2, "ecel": 3, 
                    "ched": 4, "rhed": 5, "srow": 6, "nl": 7, "ucel": 8,
                    "lcel": 9, "xcel": 10
                }
                self._init_data = {"word_map": {"word_map_tag": word_map}}
                self._prof = False
        
        # Create mock model and decoder
        model = MockModel()
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        decoder = BatchedTableDecoderV2(model, device)
        
        # Test with different max_steps values
        test_cases = [8, 16, 32]
        
        for max_steps in test_cases:
            print(f"\\nTesting max_steps={max_steps}")
            
            # Create test data
            enc_out_batch, mem_enc = create_test_data(B=2)
            
            # Test eager path
            os.environ["USE_CUDA_GRAPHS"] = "0"
            torch.manual_seed(42)
            if torch.cuda.is_available():
                torch.cuda.manual_seed(42)
            
            try:
                eager_results = decoder.predict_batched(
                    enc_out_batch.clone(), mem_enc.clone(), max_steps
                )
                print(f"  Eager: {len(eager_results)} sequences")
                for i, (seq, _, _) in enumerate(eager_results[:2]):  # Show first 2
                    print(f"    Sample {i}: length={len(seq)}, seq={seq[:5]}...")
            except Exception as e:
                print(f"  Eager failed: {e}")
                continue
            
            # Test graph path
            os.environ["USE_CUDA_GRAPHS"] = "1"
            torch.manual_seed(42)
            if torch.cuda.is_available():
                torch.cuda.manual_seed(42)
            
            try:
                graph_results = decoder.predict_batched(
                    enc_out_batch.clone(), mem_enc.clone(), max_steps
                )
                print(f"  Graph: {len(graph_results)} sequences")
                for i, (seq, _, _) in enumerate(graph_results[:2]):  # Show first 2
                    print(f"    Sample {i}: length={len(seq)}, seq={seq[:5]}...")
                
                # Compare sequences
                if len(eager_results) == len(graph_results):
                    all_match = True
                    for i, ((e_seq, _, _), (g_seq, _, _)) in enumerate(zip(eager_results, graph_results)):
                        if e_seq != g_seq:
                            print(f"    ‚ùå Sample {i} sequences differ!")
                            print(f"      Eager: {e_seq}")
                            print(f"      Graph: {g_seq}")
                            all_match = False
                    
                    if all_match:
                        print(f"  ‚úÖ All sequences match for max_steps={max_steps}")
                    else:
                        print(f"  ‚ùå Sequences differ for max_steps={max_steps}")
                else:
                    print(f"  ‚ùå Different number of results: {len(eager_results)} vs {len(graph_results)}")
                    
            except Exception as e:
                print(f"  Graph failed: {e}")
                continue
        
        print("\\n=== Correctness Test Complete ===")
        
    except ImportError as e:
        print(f"Could not import decoder: {e}")
        print("Make sure you're running from the correct directory")
        return False
    
    return True

def test_performance():
    """Test performance improvement from graphs"""
    print("\\n=== Testing Performance ===")
    
    try:
        from fork.table.batched_graph_decoder import BatchedTableDecoderV2
        
        # Create a more realistic mock for performance testing
        # (This would need actual model components in real testing)
        print("Performance testing requires full model - skipping for now")
        print("Run with actual model data to measure speedup")
        
    except ImportError:
        print("Decoder not available for performance testing")
    
    return True

if __name__ == "__main__":
    print("Testing CUDA Graphs fixes...")
    
    success = True
    success &= test_correctness()
    success &= test_performance()
    
    if success:
        print("\\nüéâ All tests completed!")
    else:
        print("\\n‚ö†Ô∏è Some tests failed")
    
    exit(0 if success else 1)